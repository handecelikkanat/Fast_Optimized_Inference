{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax device count: 1\n",
      "jax local device count:  1\n",
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jax\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# JAX setup\n",
    "JAX_SEED=42\n",
    "print('jax device count:', jax.device_count())  # total number of accelerator devices in the cluster\n",
    "print('jax local device count: ', jax.local_device_count())  # number of accelerator devices attached to this host\n",
    "\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyS6rRC_lgGY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simple Speed Comparison: BERT vs JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d1ndJdHjHqg",
    "outputId": "a82dfbb9-1d11-4d42-b77f-67c5692ca15a"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, FlaxBertModel\n",
    "import jax\n",
    "from jax import grad, jit\n",
    "import jax.numpy as np\n",
    "np.set_printoptions(linewidth=240)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "jax_model = FlaxBertModel.from_pretrained('bert-base-uncased')\n",
    "pt_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTxoftzOjVyE",
    "outputId": "47e548d1-eb03-4819-cf48-16daebd62b3c"
   },
   "outputs": [],
   "source": [
    "def pt_forward():\n",
    "    inputs = tokenizer(\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\", return_tensors=\"pt\")\n",
    "    outputs = pt_model(**inputs)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "pt_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGSMrQjzjpuy",
    "outputId": "e0fb66f9-2ba2-485f-8872-5343ce8f28e3"
   },
   "outputs": [],
   "source": [
    "%timeit pt_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjZhZ9qGjqui",
    "outputId": "a4d83d32-10ee-44a9-edd7-f39c34e34454"
   },
   "outputs": [],
   "source": [
    "def jax_forward():\n",
    "    inputs = tokenizer(\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\", return_tensors='jax')\n",
    "    outputs = jit(jax_model)(**inputs)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "jax_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58TgXoPdjt0x",
    "outputId": "9d0bd12a-2f6d-4072-cc40-2892640a67d1"
   },
   "outputs": [],
   "source": [
    "%timeit jax_forward().block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R69buRHmFA9"
   },
   "source": [
    "### Langid Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eiu3UnjmHKG",
    "outputId": "152e9215-de2e-40a8-ea83-68c8d58611ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Text', 'Language'],\n",
      "        num_rows: 9303\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Text', 'Language'],\n",
      "        num_rows: 1034\n",
      "    })\n",
      "})\n",
      "['English', 'German', 'Dutch', 'Tamil', 'Greek', 'Greek', 'French', 'Spanish', 'Russian', 'Malayalam']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from difflib import get_close_matches\n",
    "\n",
    "DATAPATH = '../../../data/langid/language_detection.csv'\n",
    "\n",
    "DATA_PERCENT_LIMIT = 100\n",
    "TEST_SPLIT = 0.1\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "split = f'train[:{DATA_PERCENT_LIMIT}%]' if DATA_PERCENT_LIMIT else 'train'\n",
    "dataset = load_dataset(\"csv\", split=split, data_files=DATAPATH, encoding='utf-8').shuffle(seed=SEED)\n",
    "dataset = dataset.train_test_split(test_size=TEST_SPLIT, seed=SEED)\n",
    "\n",
    "N_LABELS = len(set(dataset['train']['Language']))\n",
    "\n",
    "print(dataset)\n",
    "print(dataset['train'][:10]['Language'])\n",
    "\n",
    "LANG2ID = {\n",
    "    'English': 0,\n",
    "    'Malayalam': 1,\n",
    "    'Hindi': 2,\n",
    "    'Tamil': 3,\n",
    "    'Kannada': 4,\n",
    "    'French': 5,\n",
    "    'Spanish': 6,\n",
    "    'Portuguese': 7,\n",
    "    'Italian': 8,\n",
    "    'Russian': 9,\n",
    "    'Sweedish': 10,\n",
    "    'Dutch': 11,\n",
    "    'Arabic': 12,\n",
    "    'Turkish': 13,\n",
    "    'German': 14,\n",
    "    'Danish': 15,\n",
    "    'Greek': 16\n",
    "    }\n",
    "\n",
    "def lang_to_id(lang):\n",
    "      return LANG2ID[get_close_matches(lang, LANG2ID.keys())[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518,
     "referenced_widgets": [
      "9cc887b025b24cff8b4b8184e99ed639",
      "43710f566333424c9185ceba18ff3ee7",
      "6d6ed1bfb29749fca673c486d2d0a824",
      "5e95917331af48c09faf9d1dfbafe915",
      "a1835e1a5d884e80a126b419a69b361b",
      "45037c38977c4a11ad72ac83d2fe993f",
      "423ec32b0962400583d6c00970ca8cfc",
      "60ce3dd895e5468aa7fdfe9845495624",
      "c328efe9a61e476a9b1f6880c57557ef",
      "d2ff778f59c84c5f84e762fb50000db1",
      "a0f7b3b956574e4b8cd0f9ff1c7a4ae5",
      "fb33d0aad8704f4da44e22096bfe7eaf",
      "c4c9e86eae6b4e4883061ec8f59305ac",
      "7aaa0aea039542e1b9438ef1336a55e0",
      "abf65fd7765d425aa9749fe7dbe28403",
      "15c06211b1fc47718006c09c951ba009",
      "fd0de6f58b414f0d9e156e5a7610d862",
      "3f431a8aa6924f25ae14e1d432dca235",
      "7d159038fd204086b153b120333bcc4f",
      "504a496205ee409ead86ee59a55ec565",
      "33aa37bed5e948138942773a41efdb10",
      "e4e03a2a6c654128a6719fee1c2fdc8a"
     ]
    },
    "id": "bkitOldIlNYa",
    "outputId": "1cf8a6af-31ad-4afa-812e-93f29d24ad4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Text', 'Language', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 9303\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Text', 'Language', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1034\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "''' BERT tokenize '''\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    batch = tokenizer(examples['Text'], padding=\"max_length\", truncation=True)\n",
    "    batch['labels'] = [lang_to_id(lang) for lang in examples['Language']]\n",
    "    return batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40548-Eal6E-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PT BERT Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Psbf9FZ-5YpR",
    "outputId": "5a36fc92-720c-44e3-bdf7-a0774ae7a498"
   },
   "outputs": [],
   "source": [
    "''' PT train '''\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "'''\n",
    "TRAIN_STEPS_LIMIT = -1\n",
    "N_EPOCHS = 1\n",
    "\n",
    "OUTPUT_PATH = '../models/pt'\n",
    "\n",
    "# Free memory\n",
    "gc.collect()\n",
    "\n",
    "# load pre-trained\n",
    "pt_model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=N_LABELS)\n",
    "\n",
    "# fine-tune\n",
    "log_dir = os.path.join(OUTPUT_PATH, datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "try:\n",
    "    os.system(f'mkdir {log_dir}')\n",
    "except:\n",
    "    print('log dir exists, aborting')\n",
    "    sys.exit(1)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=log_dir,\n",
    "                                  label_names=['labels'],\n",
    "                                  num_train_epochs=N_EPOCHS,\n",
    "                                  max_steps = TRAIN_STEPS_LIMIT, #overrides num_train_epochs\n",
    "                                  per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "                                  per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "                                  eval_strategy=\"steps\",\n",
    "                                  eval_steps=500)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "pt_trainer = Trainer(\n",
    "    model=pt_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "pt_train_start_time = time.time()\n",
    "pt_trainer.train()\n",
    "pt_train_end_time = time.time()\n",
    "pt_train_time = pt_train_end_time - pt_train_start_time      # PT train time: 1044.08ms per iteration (9713.03s / 9303 data points)\n",
    "'''\n",
    "\n",
    "pt_train_time = 9713.03 #seconds\n",
    "print(f\"PT train time: {pt_train_time / len(tokenized_datasets['train']) * 1000:0.2f}ms per iteration \"\n",
    "      f\"({pt_train_time:0.2f}s / {len(tokenized_datasets['train'])} data points)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PT BERT Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' PT evaluate '''\n",
    "\n",
    "CHECKPOINT = '../../models/langid/pt/20250318-0945/checkpoint-2326'\n",
    "\n",
    "# load saved checkpoint\n",
    "pt_checkpoint = BertForSequenceClassification.from_pretrained(CHECKPOINT, num_labels=N_LABELS)\n",
    "\n",
    "pt_eval_trainer = Trainer(\n",
    "    model=pt_checkpoint,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "pt_eval_start_time = time.time()\n",
    "pt_eval_trainer.evaluate()\n",
    "pt_eval_end_time = time.time()\n",
    "pt_eval_time = pt_eval_end_time - pt_eval_start_time       # PT eval time: 378.33ms per iteration (391.20s / 1034 data points) \n",
    "\n",
    "print(f\"PT eval time: {pt_eval_time / len(tokenized_datasets['test']) * 1000:0.2f}ms per iteration \"\n",
    "      f\"({pt_eval_time:0.2f}s / {len(tokenized_datasets['test'])} data points)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX BERT Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Setup '''\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "import flax\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import traverse_util\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
    "\n",
    "N_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Free memory\n",
    "gc.collect()\n",
    "\n",
    "# setup\n",
    "num_train_steps = len(dataset['train']) // TRAIN_BATCH_SIZE * N_EPOCHS\n",
    "learning_rate_function = optax.linear_schedule(init_value=LEARNING_RATE, end_value=0, transition_steps=num_train_steps)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    logits_function: Callable = flax.struct.field(pytree_node=False)\n",
    "    loss_function: Callable = flax.struct.field(pytree_node=False)\n",
    "\n",
    "def decay_mask_fn(params):\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n",
    "    return traverse_util.unflatten_dict(flat_mask)\n",
    "\n",
    "def adamw(weight_decay):\n",
    "    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)\n",
    "\n",
    "def loss_function(logits, labels):\n",
    "  xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=N_LABELS))\n",
    "  return jnp.mean(xentropy)\n",
    "     \n",
    "def eval_function(logits):\n",
    "    return logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'scale')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel'), ('bert', 'pooler', 'dense', 'bias'), ('bert', 'pooler', 'dense', 'kernel')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxBertForSequenceClassification, BertConfig\n",
    "\n",
    "# load pre-trained\n",
    "config = BertConfig.from_pretrained('google-bert/bert-base-cased', num_labels=N_LABELS)\n",
    "jax_model = FlaxBertForSequenceClassification.from_pretrained('google-bert/bert-base-cased', config=config, seed=JAX_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainState.create(\n",
    "    apply_fn=jax_model.__call__,\n",
    "    params=jax_model.params,\n",
    "    tx=adamw(weight_decay=0.01),\n",
    "    logits_function=eval_function,\n",
    "    loss_function=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch, dropout_rng):\n",
    "    targets = batch.pop(\"labels\")\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "\n",
    "    def loss_function(params):\n",
    "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
    "        loss = state.loss_function(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    grad_function = jax.value_and_grad(loss_function)\n",
    "    loss, grad = grad_function(state.params)\n",
    "    grad = jax.lax.pmean(grad, \"batch\")\n",
    "    new_state = state.apply_gradients(grads=grad)\n",
    "    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n",
    "    return new_state, metrics, new_dropout_rng\n",
    "\n",
    "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch):\n",
    "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
    "    return state.logits_function(logits)\n",
    "\n",
    "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loader(rng, dataset, batch_size):\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        del batch['Text']\n",
    "        del batch['Language']\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "        yield batch\n",
    "\n",
    "def eval_data_loader(dataset, batch_size):\n",
    "    for i in range(len(dataset) // batch_size):\n",
    "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
    "        del batch['Text']\n",
    "        del batch['Language']        \n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Initialize '''\n",
    "\n",
    "state = flax.jax_utils.replicate(state)\n",
    "num_labels = flax.jax_utils.replicate(N_LABELS)\n",
    "\n",
    "rng = jax.random.PRNGKey(JAX_SEED)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "#for i, epoch in enumerate(tqdm(range(1, N_EPOCHS + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
    "rng, input_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' JAX train '''\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "\n",
    "jax_train_start_time = time.time()\n",
    "with tqdm(total=len(tokenized_datasets['train']) // TRAIN_BATCH_SIZE, desc=\"Training...\", leave=False) as progress_bar_train:\n",
    "  for batch in train_data_loader(input_rng, tokenized_datasets['train'], TRAIN_BATCH_SIZE):\n",
    "    state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
    "    progress_bar_train.update(1)\n",
    "jax_train_end_time = time.time()\n",
    "jax_train_time = jax_train_end_time - jax_train_start_time ## Jax train time: 44.10ms per iteration (410.22s / 9303 data points)\n",
    "\n",
    "print(f\"Jax train time: {jax_train_time / len(tokenized_datasets['train']) * 1000:0.2f}ms per iteration \"\n",
    "      f\"({jax_train_time:0.2f}s / {len(tokenized_datasets['train'])} data points)\")\n",
    "\n",
    "# save model\n",
    "OUTPUT_PATH = '/home/hande/Work/Fast_Optimized_Inference/models/JAX/langid/jax'\n",
    "log_dir = os.path.join(OUTPUT_PATH, datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "try:\n",
    "    os.system(f'mkdir {log_dir}')\n",
    "except:\n",
    "    print('log dir exists')\n",
    "\n",
    "jax_model.save_pretrained(log_dir)\n",
    "tokenizer.save_pretrained(log_dir)\n",
    "\n",
    "# Save parameters using Orbax (recommended over pickle)\n",
    "checkpointer = ocp.PyTreeCheckpointer()\n",
    "checkpointer.save(os.path.join(log_dir, 'params'), state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX BERT Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sharding of jax.Array cannot be None. Provide `mesh` and `mesh_axes` OR `sharding`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     22\u001b[39m checkpointer = ocp.PyTreeCheckpointer()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#jax_params = checkpointer.restore(os.path.join(CHECKPOINT, 'params'))\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Assign loaded parameters to the model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#jax_checkpoint.params = state.params\u001b[39;00m\n\u001b[32m     31\u001b[39m checkpoint_state = TrainState.create(\n\u001b[32m     32\u001b[39m     apply_fn=jax_checkpoint.\u001b[34m__call__\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     params=\u001b[43mcheckpointer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     34\u001b[39m     tx=adamw(weight_decay=\u001b[32m0.01\u001b[39m),\n\u001b[32m     35\u001b[39m     logits_function=eval_function,\n\u001b[32m     36\u001b[39m     loss_function=loss_function,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     38\u001b[39m checkpoint_state = flax.jax_utils.replicate(checkpoint_state)\n\u001b[32m     40\u001b[39m jax_tokenizer = BertTokenizer.from_pretrained(CHECKPOINT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/checkpointers/checkpointer.py:289\u001b[39m, in \u001b[36mCheckpointer.restore\u001b[39m\u001b[34m(self, directory, *args, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m logging.info(\u001b[33m'\u001b[39m\u001b[33mRestoring checkpoint from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, directory)\n\u001b[32m    288\u001b[39m ckpt_args = construct_checkpoint_args(\u001b[38;5;28mself\u001b[39m._handler, \u001b[38;5;28;01mFalse\u001b[39;00m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m restored = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m multihost.sync_global_processes(\n\u001b[32m    291\u001b[39m     multihost.unique_barrier_key(\n\u001b[32m    292\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mCheckpointer:restore\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     processes=\u001b[38;5;28mself\u001b[39m._active_processes,\n\u001b[32m    296\u001b[39m )\n\u001b[32m    297\u001b[39m restore_duration_secs = time.time() - restore_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/checkpointers/checkpointer.py:308\u001b[39m, in \u001b[36mCheckpointer._restore\u001b[39m\u001b[34m(self, directory, args)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_restore\u001b[39m(\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m, directory: epath.PathLike, args: checkpoint_args.CheckpointArgs\n\u001b[32m    307\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/handlers/pytree_checkpoint_handler.py:796\u001b[39m, in \u001b[36mPyTreeCheckpointHandler.restore\u001b[39m\u001b[34m(self, directory, item, restore_args, transforms, transforms_default_to_original, legacy_transform_fn, args)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    787\u001b[39m     (directory / PYTREE_METADATA_FILE).exists()\n\u001b[32m    788\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m can_ignore_aggregate_file\n\u001b[32m    789\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m legacy_transform_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    791\u001b[39m ):\n\u001b[32m    792\u001b[39m   args = BasePyTreeRestoreArgs(\n\u001b[32m    793\u001b[39m       item,\n\u001b[32m    794\u001b[39m       restore_args=restore_args,\n\u001b[32m    795\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handler_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m logging.vlog(\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdirectory=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, restore_args=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m, directory, restore_args)\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directory.exists():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/handlers/base_pytree_checkpoint_handler.py:736\u001b[39m, in \u001b[36mBasePyTreeCheckpointHandler.restore\u001b[39m\u001b[34m(self, directory, args)\u001b[39m\n\u001b[32m    728\u001b[39m param_infos = \u001b[38;5;28mself\u001b[39m._get_param_infos(\n\u001b[32m    729\u001b[39m     item=value_metadata_tree,\n\u001b[32m    730\u001b[39m     directory=directory,\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m     raise_array_data_missing_error=raise_array_data_missing_error,\n\u001b[32m    734\u001b[39m )\n\u001b[32m    735\u001b[39m \u001b[38;5;66;03m# Begin restore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m tree_memory_size, restored_item = \u001b[43masyncio_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_deserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_metadata_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_args\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging.vlog_is_on(\u001b[32m1\u001b[39m):\n\u001b[32m    743\u001b[39m   logging.vlog(\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mparam_infos: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m, param_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/asyncio_utils.py:50\u001b[39m, in \u001b[36mrun_sync\u001b[39m\u001b[34m(coro, enable_nest_asyncio)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m     49\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/tasks.py:279\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         result = coro.throw(exc)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    282\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/handlers/base_pytree_checkpoint_handler.py:584\u001b[39m, in \u001b[36mBasePyTreeCheckpointHandler._maybe_deserialize\u001b[39m\u001b[34m(self, item, metadata, param_infos, restore_args)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m request \u001b[38;5;129;01min\u001b[39;00m batch_requests:\n\u001b[32m    581\u001b[39m   deserialized_batches_ops.append(\n\u001b[32m    582\u001b[39m       request.handler.deserialize(request.infos, request.args)\n\u001b[32m    583\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m deserialized_batches += \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*deserialized_batches_ops)\n\u001b[32m    586\u001b[39m tree_memory_size = \u001b[32m0\u001b[39m\n\u001b[32m    587\u001b[39m flat_restored = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/langid/lib/python3.11/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1276\u001b[39m, in \u001b[36mArrayHandler.deserialize\u001b[39m\u001b[34m(self, infos, args)\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mUnable to deserialize sharding.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1277\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mSharding of jax.Array cannot be None. Provide `mesh`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1278\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m and `mesh_axes` OR `sharding`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1279\u001b[39m   )\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info.is_ocdbt_checkpoint:\n\u001b[32m   1281\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m _assert_parameter_files_exist(\n\u001b[32m   1282\u001b[39m       info.path,\n\u001b[32m   1283\u001b[39m       \u001b[38;5;28mself\u001b[39m._metadata_key,\n\u001b[32m   1284\u001b[39m       info.use_zarr3,\n\u001b[32m   1285\u001b[39m   )\n",
      "\u001b[31mValueError\u001b[39m: Sharding of jax.Array cannot be None. Provide `mesh` and `mesh_axes` OR `sharding`"
     ]
    }
   ],
   "source": [
    "''' JAX evaluate '''\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from jax import jit\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "from transformers import FlaxBertForSequenceClassification, BertConfig, BertTokenizer\n",
    "import evaluate\n",
    "\n",
    "\n",
    "CHECKPOINT = '/home/hande/Work/Fast_Optimized_Inference/models/JAX/langid/jax/20250318-2352'\n",
    "\n",
    "# load saved checkpoint\n",
    "config = BertConfig.from_pretrained(CHECKPOINT, num_labels=N_LABELS)\n",
    "jax_checkpoint = FlaxBertForSequenceClassification.from_pretrained(CHECKPOINT, config=config)\n",
    "\n",
    "# Load parameters using Orbax\n",
    "checkpointer = ocp.PyTreeCheckpointer()\n",
    "#jax_params = checkpointer.restore(os.path.join(CHECKPOINT, 'params'))\n",
    "\n",
    "# Assign loaded parameters to the model\n",
    "# jax_checkpoint.params = jax_params\n",
    "# jax_checkpoint_jitted = jit(jax_checkpoint)\n",
    "\n",
    "\n",
    "#jax_checkpoint.params = state.params\n",
    "checkpoint_state = TrainState.create(\n",
    "    apply_fn=jax_checkpoint.__call__,\n",
    "    params=checkpointer.restore(os.path.join(CHECKPOINT, 'params')),\n",
    "    tx=adamw(weight_decay=0.01),\n",
    "    logits_function=eval_function,\n",
    "    loss_function=loss_function,\n",
    ")\n",
    "checkpoint_state = flax.jax_utils.replicate(checkpoint_state)\n",
    "\n",
    "jax_tokenizer = BertTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "jax_acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# evaluate\n",
    "jax_eval_start_time = time.time()\n",
    "\n",
    "'''\n",
    "for data in tqdm(tokenized_datasets['test']):\n",
    "    #print(data)\n",
    "    texts = data['Text']\n",
    "    labels = [lang_to_id(data['Language'])]\n",
    "    inputs = jax_tokenizer(texts, return_tensors='jax', padding=\"max_length\", truncation=True)\n",
    "    predictions = [eval_function(jax_checkpoint_jitted(**inputs)['logits'])]\n",
    "    #print(predictions, labels)\n",
    "    jax_acc_metric.add_batch(predictions=predictions, references=labels)\n",
    "'''\n",
    "\n",
    "with tqdm(total=len(tokenized_datasets['test']) // EVAL_BATCH_SIZE, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "  for batch in eval_data_loader(tokenized_datasets['test'], EVAL_BATCH_SIZE):\n",
    "      labels = [batch.pop(\"labels\")]\n",
    "      predictions = parallel_eval_step(checkpoint_state, batch)\n",
    "      #predictions = [parallel_eval_step(state, batch)]\n",
    "      jax_acc_metric.add_batch(predictions=predictions, references=labels)\n",
    "      progress_bar_eval.update(1)\n",
    "\n",
    "jax_eval_end_time = time.time()\n",
    "jax_eval_time = jax_eval_end_time - jax_eval_start_time     #Jax eval time: 19.69ms per iteration (20.36s / 1034 data points)\n",
    "\n",
    "print(f\"Jax eval time: {jax_eval_time / len(tokenized_datasets['test']) * 1000:0.2f}ms per iteration \"\n",
    "      f\"({jax_eval_time:0.2f}s / {len(tokenized_datasets['test'])} data points)\")\n",
    "\n",
    "jax_eval_acc_metric = jax_acc_metric.compute()\n",
    "\n",
    "#loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
    "jax_eval_acc_score = round(list(jax_eval_acc_metric.values())[0], 3)  #Eval accuracy: 0.918\n",
    "metric_name = list(jax_eval_acc_metric.keys())[0]\n",
    "\n",
    "print(f\"Eval {metric_name}: {jax_eval_acc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjCBNlOJntdV"
   },
   "source": [
    "### Fasttext Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import fasttext\n",
    "from difflib import get_close_matches\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "'''\n",
    "LANG2ID = {\n",
    "    '__label__eng': 0,\n",
    "    'Malayalam': 1,\n",
    "    'Hindi': 2,\n",
    "    'Tamil': 3,\n",
    "    'Kannada': 4,\n",
    "    'French': 5,\n",
    "    'Spanish': 6,\n",
    "    'Portuguese': 7,\n",
    "    'Italian': 8,\n",
    "    'Russian': 9,\n",
    "    'Sweedish': 10,\n",
    "    'Dutch': 11,\n",
    "    'Arabic': 12,\n",
    "    'Turkish': 13,\n",
    "    'German': 14,\n",
    "    'Danish': 15,\n",
    "    'Greek': 16\n",
    "    }\n",
    "\n",
    "def lang_to_id(lang):\n",
    "      return LANG2ID[get_close_matches(lang, LANG2ID.keys())[0]]\n",
    "\n",
    "\n",
    "'__label__eng_Latn'\n",
    "af als am an ar arz as ast av az azb ba bar bcl be bg bh bn bo bpy br bs bxr ca cbk ce ceb ckb co cs cv cy da de diq dsb dty dv el eml en eo es et eu fa fi fr frr fy ga gd gl gn gom gu gv he hi hif hr hsb ht hu hy ia id ie ilo io is it ja jbo jv ka kk km kn ko krc ku kv kw ky la lb lez li lmo lo lrc lt lv mai mg mhr min mk ml mn mr mrj ms mt mwl my myv mzn nah nap nds ne new nl nn no oc or os pa pam pfl pl pms pnb ps pt qu rm ro ru rue sa sah sc scn sco sd sh si sk sl so sq sr su sv sw ta te tg th tk tl tr tt tyv ug uk ur uz vec vep vi vls vo wa war wuu xal xmf yi yo yue zh\n",
    "'''\n",
    "\n",
    "fasttext_model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", \n",
    "                                      filename=\"model.bin\", \n",
    "                                      cache_dir=\"../../models/langid/fasttext/cached\")\n",
    "fasttext_model = fasttext.load_model(fasttext_model_path)\n",
    "\n",
    "fasttest_acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "fasttext_eval_start_time = time.time()\n",
    "for data in tqdm(tokenized_datasets['test']):\n",
    "    input = data['Text']\n",
    "    prediction = fasttext_model.predict(input)\n",
    "    fasttext_acc_metric.add_batch(predictions=[prediction], references=[lang_to_id(data['Language'])])\n",
    "fasttext_eval_end_time = time.time()\n",
    "\n",
    "fasttext_eval_acc_metric = fasttext_acc_metric.compute()\n",
    "fasttext_eval_acc_score = round(list(fasttext_metric.values())[0], 3)\n",
    "\n",
    "print(f\"fasttext eval {metric_name}: {fasttext_eval_acc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85fVmkmll_ge"
   },
   "source": [
    "### Langid Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YIzFI3gmArN"
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN IN PARALLEL -- BATCHES OF 1!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GH8Nw0dm0x6"
   },
   "source": [
    "### Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKNMI8qim5kh"
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "stats_list = {'pt': [pt_train_end_time - pt_train_start_time,\n",
    "                     (pt_eval_end_time - pt_eval_start_time) / len(tokenized_datasets['test'],\n",
    "                     pt_eval_acc_score,\n",
    "                     pt_eval_roc_auc_score],\n",
    "              'jax': [jax_train_end_time - jax_train_start_time,\n",
    "                      (jax_eval_end_time - jax_eval_start_time) / len(tokenized_datasets['test'],\n",
    "                      jax_eval_acc_score,\n",
    "                      jax_eval_roc_auc_score]\n",
    "              'fasttext': [fasttext_train_end_time - fasttext_train_start_time,\n",
    "                    (fasttext_eval_end_time - fasttext_eval_start_time) / len(tokenized_datasets['test'],\n",
    "                    fasttext_eval_acc_score,\n",
    "                    fasttext_eval_roc_auc_score]                    \n",
    "                    \n",
    "                    'jax': ['a', 'b', 'c', 'd'], 'fasttext': [0, 0]}\n",
    "stats_list_pd = pd.DataFrame.from_dict(data)\n",
    "\n",
    "runtimes = {\n",
    "    'train'= {\n",
    "        'pt': pt_train_end_time - pt_train_start_time,\n",
    "        'jax': jax_train_end_time - jax_train_start_time,\n",
    "        'fasttext': 0\n",
    "    },\n",
    "    'inference' = {\n",
    "        'pt': pt_eval_end_time - pt_evak_start_time,\n",
    "        'jax': jax_eval_end_time - jax_eval_start_time,\n",
    "        'fasttext': 0\n",
    "    }    \n",
    "}\n",
    "\n",
    "sns.barplot(penguins, x=\"island\", y=\"body_mass_g\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v13WZoU6m4b2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "agiUvmZ1lzEv",
    "kyS6rRC_lgGY"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15c06211b1fc47718006c09c951ba009": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33aa37bed5e948138942773a41efdb10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f431a8aa6924f25ae14e1d432dca235": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "423ec32b0962400583d6c00970ca8cfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43710f566333424c9185ceba18ff3ee7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45037c38977c4a11ad72ac83d2fe993f",
      "placeholder": "​",
      "style": "IPY_MODEL_423ec32b0962400583d6c00970ca8cfc",
      "value": "Map: 100%"
     }
    },
    "45037c38977c4a11ad72ac83d2fe993f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "504a496205ee409ead86ee59a55ec565": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e95917331af48c09faf9d1dfbafe915": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2ff778f59c84c5f84e762fb50000db1",
      "placeholder": "​",
      "style": "IPY_MODEL_a0f7b3b956574e4b8cd0f9ff1c7a4ae5",
      "value": " 9303/9303 [00:20&lt;00:00, 409.19 examples/s]"
     }
    },
    "60ce3dd895e5468aa7fdfe9845495624": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d6ed1bfb29749fca673c486d2d0a824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60ce3dd895e5468aa7fdfe9845495624",
      "max": 9303,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c328efe9a61e476a9b1f6880c57557ef",
      "value": 9303
     }
    },
    "7aaa0aea039542e1b9438ef1336a55e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d159038fd204086b153b120333bcc4f",
      "max": 1034,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_504a496205ee409ead86ee59a55ec565",
      "value": 1034
     }
    },
    "7d159038fd204086b153b120333bcc4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cc887b025b24cff8b4b8184e99ed639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43710f566333424c9185ceba18ff3ee7",
       "IPY_MODEL_6d6ed1bfb29749fca673c486d2d0a824",
       "IPY_MODEL_5e95917331af48c09faf9d1dfbafe915"
      ],
      "layout": "IPY_MODEL_a1835e1a5d884e80a126b419a69b361b"
     }
    },
    "a0f7b3b956574e4b8cd0f9ff1c7a4ae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1835e1a5d884e80a126b419a69b361b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abf65fd7765d425aa9749fe7dbe28403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33aa37bed5e948138942773a41efdb10",
      "placeholder": "​",
      "style": "IPY_MODEL_e4e03a2a6c654128a6719fee1c2fdc8a",
      "value": " 1034/1034 [00:03&lt;00:00, 291.80 examples/s]"
     }
    },
    "c328efe9a61e476a9b1f6880c57557ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4c9e86eae6b4e4883061ec8f59305ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd0de6f58b414f0d9e156e5a7610d862",
      "placeholder": "​",
      "style": "IPY_MODEL_3f431a8aa6924f25ae14e1d432dca235",
      "value": "Map: 100%"
     }
    },
    "d2ff778f59c84c5f84e762fb50000db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4e03a2a6c654128a6719fee1c2fdc8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb33d0aad8704f4da44e22096bfe7eaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c4c9e86eae6b4e4883061ec8f59305ac",
       "IPY_MODEL_7aaa0aea039542e1b9438ef1336a55e0",
       "IPY_MODEL_abf65fd7765d425aa9749fe7dbe28403"
      ],
      "layout": "IPY_MODEL_15c06211b1fc47718006c09c951ba009"
     }
    },
    "fd0de6f58b414f0d9e156e5a7610d862": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
